import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM

model_path = "/home/mengchen/models/chatglm3-6b-hf"
model_path = "/home/mengchen/models/Llama-2-7b-chat-hf"
model_path = "/home/mengchen/models/Baichuan-13B-Chat"
model_path = "/home/mengchen/models/Baichuan2-13B-Chat"
model_path = "/home/mengchen/models/Qwen2.5-14B-Instruct"
model_path = "/home/mengchen/models/deepseek-coder-33b-instruct"

import pdb
pdb.set_trace()

device = torch.device('cpu:0')
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
#model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device=device)
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).bfloat16().to(device)
#model = model.eval()

input1 = "SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models\nfor Private and Secure Inference\nAbhijit Mishra, Mingda Li, Soham Deo\nSchool of Information\nUniversity of Texas at Austin\nabhijitmishra, mingdali, soham.deo@utexas.edu\nAbstract\nThis paper addresses the privacy and security concerns as-\nsociated with deep neural language models, which serve\nas crucial components in various modern AI-based applica-\ntions. These models are often used after being pre-trained\nand fine-tuned for specific tasks, with deployment on servers\naccessed through the internet. However, this introduces two\nfundamental risks: (a) the transmission of user inputs to the\nserver via the network gives rise to interception vulnera-\nbilities, and (b) privacy concerns emerge as organizations\nthat deploy such models store user data with restricted con-\ntext. To address this, we propose a novel method to adapt\nand fine-tune transformer-based language models on passkey-\n\ning additional pre-training. This approach diverges from\nconventional practices in cybersecure language model\ndevelopment.\n• Enhanced Text Security: We enhance text security in\nthe adapted models by implementing systematic tech-\nniques such as tokenizer encryption, token shuffling, and\ntoken embedding transformation. These measures collec-\ntively make it challenging to reverse engineer the original\ntext from the tokens and embeddings.\n• Empirical Validation: Through extensive evaluation on\nvarious NLP benchmark datasets, we demonstrate that\nthe overall transformation process maintains a lossless\nperformance while being cost-effective. Additionally, the\nprocess is designed for easy replication to address new\nencryption requirements.\nThe code for this paper, is available at https://github.com/\nabhijitmishra/sentinellm-aaai2024.\n2\nRelated Work\nDeep\nneural\nnetwork\nlanguage\nmodels\nlike\nBERT,\nRoBERTa, and GPT have gained significant prominence in\n\nnerlein, C. 2013. BLAKE2: simpler, smaller, fast as MD5.\nIn Applied Cryptography and Network Security: 11th Inter-\nnational Conference, ACNS 2013, Banff, AB, Canada, June\n25-28, 2013. Proceedings 11, 119–135. Springer.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nCallegati, F.; Cerroni, W.; and Ramilli, M. 2009. Man-in-\nthe-Middle Attack to the HTTPS Protocol. IEEE Security &\nPrivacy, 7(1): 78–81.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nConneau, A.; Lample, G.; Rinott, R.; Williams, A.; Bow-\nman, S. R.; Schwenk, H.; and Stoyanov, V. 2018. XNLI:\nEvaluating cross-lingual sentence representations.\narXiv\n\n根据以上内容来回答下面的问题。如果你不知道答案，就说你不知道，不要试图编造答案。尽量使答案简明扼要，语句通顺。\n请你在回答中使用与问题相同的语言，并且总是在回答的最后加上\"谢谢你的提问!\"。\n\n问题：what is sentinelLMs\n\n下面请给出你的回答，禁止自问自答。"
input2 = "1981年6月29日，邓小平在十一届六中全会闭幕会上讲话，指出通过《关于建国以来党的若干历史问题的决议》，\n对统一党内思想意义很大。\n邓小平的第二项工作分为四个步骤：第一步，确定“社会主义现代化”为中国的奋斗目\n标；第二步，于1982年提出了“有中国特色社会主义”的概念。他解释说，“中国特色社会\n主义”就是从“中国实际出发”，“走自己的路”。很明显，提出这一口号的目的是要唤起人\n们的爱国心；第三步是在1983年提出了“改革”的思想。这比他（及其他人）以往所说\n的“调整”要更为广泛和深刻。最后，是提出了完整的改革理论。这一理论同时涵盖了政治\n和经济两个方面及二者之间的关系。邓小平认为，政治改革有赖于经济改革，这与马克思\n经济基础决定政治和文化上层建筑的论断是一致的，但有悖于毛泽东最后20年的理论。邓\n小平对社会主义本质的描述一直非常谨慎，他没有贸然用符合马克思主义思想的名词给社\n会主义下一个完整的定义，因而许多党的领导人不太容易准确理解他的思想，当一些外国\n人说邓小平把中国引向了资本主义时，他们很恼火。\n实际上，早在邓小平提出他的改革理论之前，中国的改革就已经开始了。邓提出“有\n中国特色社会主义”之前，农村改革就已如火如荼。政府提高了农产品的收购价格，冻结\n了每项产品的收购额度；更重要的是政府恢复了农民以家庭为主从事生产的权力：土地仍\n归集体所有，但归农民承包使用。根据土地承包合同，农民有长期使用土地的权利，但要\n向国家和集体上交一部分产品，其余的部分可以卖给国家也可以拿到市场上出售。全国范\n围内“家庭联产承包制”的实施带来了中国农村的新格局，这一格局既不同于土改后的50年\n代，也不同于集体化时代。从理论上讲，农民只是不再从事集体劳动，但事实上，长期的\n\n1984年2月，在视察广东、福建后，肯定建立经济特区的政策是正确的，并建议增加\n对外开放的城市。\n4月，中共中央、国务院根据邓小平的意见召开沿海部分城市座谈会，并于5月4日发\n出《沿海部分城市座谈会纪要》的通知，确定进一步开放14个沿海港口城市。\n6月22日、23日，分别会见香港工商界访京团和香港知名人士钟士元等。在同他们谈\n话时指出，用“一个国家，两种制度”的办法来解决香港和台湾问题，是全国人民代表大会\n通过的政策，不会变。\n6月30日，会见中日民间人士会议日方委员会代表团。谈话时指出，社会主义阶段的\n最根本任务就是发展生产力。\n10月1日，在中华人民共和国成立35周年庆祝典礼上检阅部队并讲话。\n10月20日，中共十二届三中全会通过《中共中央关于经济体制改革的决定》。\n10月22日，在中共中央顾问委员会第三次全体会议上讲话。在谈到台湾问题时指出，\n我们坚持谋求用和平的方式解决台湾问题，但是始终没有放弃非和平方式的可能性，我们\n不能作排除使用武力的承诺。这是一种战略考虑。\n10月，多次谈话指出，中国的发展离不开世界；对内搞活经济、对外开放是根本政\n策；对内搞活经济，首先从农村着手。中国社会是不是安定，经济能不能发展，首先要看\n农村能不能发展，农民生活是不是好起来。现在改革由农村转入城市，改革包括工业、商\n业、服务业，还包括科教、文化等领域，是全面改革。\n12月19日，出席中英两国政府关于香港问题联合声明的签字仪式。\n1985年1月19日，会见香港核电投资有限公司代表团。谈话时说，中国的对外开放、\n吸引外资的政策，是一项长期持久的政策。我们的开放政策不会导致资本主义。\n3月4日，会见日本商工会议所访华团。谈话时指出，和平和发展是当代世界的两大问\n题。\n3月7日，在全国科技工作会议上作《改革科技体制是为了解放生产力》讲话。随后作\n即席讲话，强调要教育全国人民做到有理想、有道德、有文化、有纪律。\n3月28日，会见日本自由民主党副总裁二阶堂进。谈话时指出，改革是中国的第二次\n革命。\n4月15日，会见坦桑尼亚联合共和国副总统姆维尼。谈话时说，我们的经验教训最重\n要的一条，就是要搞清楚什么是社会主义，如何建设社会主义。\n5月19日，在全国教育工作会议上讲话指出，各级党和政府要把教育工作认真抓起\n来。强调一个地区、一个部门，如果只抓经济、不抓教育，那里的工作重点就是没有转移\n\n席职务。\n1991年1月至2月，视察上海。同上海市负责人谈话时提出，抓紧开发浦东，不要动\n摇，一直到建成；希望上海人民思想更解放一点，胆子更大一点，步子更快一点。\n8月20日，同中共中央几位负责人谈话时指出，坚持改革开放是决定中国命运的一\n招。\n1992年1月至2月，到武昌、深圳、珠海、上海等地视察，发表重要谈话，分析了国际\n国内形势，总结了十一届三中全会以来党的基本实践和基本经验，明确回答了经常困扰和\n束缚人们思想的许多重大认识问题。指出，计划和市场都是经济手段，不是社会主义与资\n本主义的本质区别。社会主义的本质，是解放生产力，发展生产力，消灭剥削，消除两极\n分化，最终达到共同富裕。提出判断是非的标准，主要看是否有利于发展社会主义社会的\n生产力，是否有利于增强社会主义国家的综合国力，是否有利于提高人民的生活水平。强\n调要抓住机遇，大胆改革，加快发展，坚持党的基本路线一百年不动摇。\n10月，中国共产党召开第十四次全国代表大会。会议确定经济体制改革的目标是建立\n社会主义市场经济体制，提出用邓小平建设有中国特色社会主义理论武装全党的战略任\n务。邓小平会见了出席十四大的全体代表。\n1993年11月2日，《邓小平文选》第三卷出版发行。中共中央举行学习《邓小平文\n选》第三卷报告会，江泽民发表重要讲话。\n1994年11月2日，经修订增补的《邓小平文选（1938～1965年）》、《邓小平文选\n（1975～1982年）》，改称《邓小平文选》第一卷、第二卷出版发行。\n1995年5月10日，中共中央发出《关于印发〈邓小平同志建设有中国特色社会主义理\n论学习纲要〉的通知》。《通知》指出：中央宣传部组织编写的《纲要》比较全面准确地\n反映了《邓小平文选》的思想，有助于更好地理解建设有中国特色社会主义理论的科学体\n系。要求各级党委组织干部认真深入研读邓小平著作，把《纲要》的学习纳入建设有中国\n特色社会主义理论的学习计划。\n5月26日，中共中央文献研究室编辑的《邓小平建设有中国特色社会主义论述专题摘\n编》（新编本），由中央文献出版社出版。新编本按照《邓小平文选》第二卷和第三卷的\n版本，对1993年1月编辑出版的《邓小平关于建设有中国特色社会主义的论述专题摘编》\n进行了增补修订，专题由原来的9个扩展为20个，篇幅由原来的18万字增加到32万字。\n\n根据以上内容来回答下面的问题。如果你不知道答案，就说你不知道，不要试图编造答案。尽量使答案简明扼要，语句通顺。\n请你在回答中使用与问题相同的语言，并且总是在回答的最后加上\"谢谢你的提问!\"。\n\n问题：改革开放的具体内容包括什么\n\n下面请给出你的回答，禁止自问自答。"

input3="what is the capital of America ?"
input = input3
pdb.set_trace()
#inputs = tokenizer.build_chat_input(input, history=[], role="user").input_ids
inputs = tokenizer(input, return_tensors="pt", padding=False).input_ids
#suffix = torch.tensor([[196]])
#prefix = torch.tensor([[195]])
#inputs = torch.cat((prefix, inputs, suffix), dim=1)
#eos_token_id = [tokenizer.eos_token_id, tokenizer.get_command("<|user|>"),
#                tokenizer.get_command("<|observation|>")]
outputs = model.generate(inputs, max_new_tokens=200, do_sample=False, temperature=0.0, top_k=20, top_p=0.8, repetition_penalty=1.0)
print(outputs)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
